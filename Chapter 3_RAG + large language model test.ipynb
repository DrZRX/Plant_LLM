{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import numpy as np\n",
    "! pip show anthropic\n",
    "! pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "ANTHROPIC_API_KEY=\"API_KEY\"\n",
    "OPENAI_API_KEY=\"API_KEY\"\n",
    "\n",
    "# Set your own IP to allow access\n",
    "proxy_url = 'http://----'\n",
    "proxy_port = 'xxxx' \n",
    "\n",
    "os.environ['http_proxy'] = f'{proxy_url}:{proxy_port}'\n",
    "os.environ['https_proxy'] = f'{proxy_url}:{proxy_port}'\n",
    "\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#API Usage\n",
    "client_claude = anthropic.Anthropic(api_key= ANTHROPIC_API_KEY)\n",
    "client_emb = OpenAI(api_key = OPENAI_API_KEY)\n",
    "client = OpenAI(api_key = OPENAI_API_KEY)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    return client_emb.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# Define chatbot\n",
    "def chat_with_claude_sonnet(prompt,system_prompt):\n",
    "    messages = [{\"role\": \"user\",\"content\": prompt}]\n",
    "    response = client_claude.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.5,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "    message = response.content[0].text\n",
    "\n",
    "    return message\n",
    "\n",
    "def chat_with_claude_opus(prompt,system_prompt):\n",
    "    messages = [{\"role\": \"user\",\"content\": prompt}]\n",
    "    response = client_claude.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=4000,\n",
    "        temperature=0.5,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "    message = response.content[0].text\n",
    "\n",
    "    return message\n",
    "\n",
    "def chat_with_claude_haiku(prompt,system_prompt):\n",
    "    messages = [{\"role\": \"user\",\"content\": prompt}]\n",
    "    response = client_claude.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=4000,\n",
    "        temperature=0.5,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "    message = response.content[0].text\n",
    "\n",
    "    return message\n",
    "\n",
    "def chat_with_openai(prompt, system_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",   # Replace the Openai model that needs to be tested here\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the vector set (Optional)\n",
    "with open('Path/ab_vector.json', 'r') as file:\n",
    "    sentence_vector = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d015c680",
   "metadata": {},
   "source": [
    "# Before doing the test, use Colab to pass the vector search results into the problem_vector_return.txt file in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99089b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG+LLM\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Read the original txt document\n",
    "with open(\"Path/problem_vector_return.txt \", \"r\", encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Use regular expressions to split multiple question parts\n",
    "questions = re.split(r'-{50,}', content)\n",
    "\n",
    "# Remove empty strings\n",
    "questions = [q.strip() for q in questions if q.strip()]\n",
    "\n",
    "# Define prompt words\n",
    "prompt = \"\"\"\n",
    "In the following content, Query: is followed by the question to be answered, and Knowledge: and Reference: are followed by relevant reference information and literature. Please answer according to the following requirements:\n",
    "\n",
    "1. Answer the question in Query accurately and do not deviate from the topic.\n",
    "2. Use [1], [2], [3] and other annotations to cite information in Knowledge.\n",
    "3. If there is insufficient information, you can make cautious guesses, but you must clearly mark them.\n",
    "4. Stay objective and neutral.\n",
    "5. Give an answer of appropriate length depending on the complexity of the question.\n",
    "6. Use clear and professional language and explain professional terms when necessary.\n",
    "7. List \"Reference:\" after the answer in the format: [number] literature title, journal name, PMCID/PMID/URL.\n",
    "\n",
    "Answer the question directly without any extra words, and do not use \"Query:\" and \"Knowledge:\" in your answer. You can include a \"Reference:\" section in your answer.\n",
    "\"\"\"\n",
    "\n",
    "# Create a file to store the analysis results\n",
    "with open(\"Path/Question_openai_test.txt\", \"w\", encoding='utf-8') as output_file:\n",
    "    # Process each question part\n",
    "    for i, question_part in enumerate(questions, 1):\n",
    "        # Set the maximum number of retries\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Get the answer using LLM\n",
    "                refined_answer = chat_with_claude_opus(question_part, prompt)\n",
    "                \n",
    "                # Write the question part and refined answer to the file\n",
    "                output_file.write(f\"Question Part {i}:\\n{question_part}\\n\")\n",
    "                output_file.write(f\"### Response:\\n{refined_answer}\\n\\n\")\n",
    "                \n",
    "                print(f\"Resolved issues {i}/{len(questions)}\")\n",
    "                break  \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"Request failed, error message: {str(e)}\")\n",
    "                print(f\"number of retries: {retry_count}/{max_retries}\")\n",
    "                \n",
    "                if retry_count == max_retries:\n",
    "                    print(f\"The maximum number of retries has been reached, skipping the question {i}\")\n",
    "                else:\n",
    "                    wait_time = 2 ** (retry_count - 1) \n",
    "                    print(f\"Wait {wait_time} seconds before trying again...\")\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd644be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only LLM _ Tests the ability to answer questions only by LLMs\n",
    "import time\n",
    "\n",
    "# Define prompt words\n",
    "prompt1 = \"\"\"\n",
    "You are an AI assistant tasked with answering questions accurately and concisely. Please follow these guidelines:\n",
    "\n",
    "1. Answer the question directly and accurately, staying on topic.\n",
    "2. Use clear, professional language. Explain technical terms if necessary.\n",
    "3. If information is insufficient, you may make cautious inferences, but clearly label them as such.\n",
    "4. Maintain objectivity and neutrality.\n",
    "5. Adjust the length of your answer based on the complexity of the question.\n",
    "6. Use [1], [2], [3], etc. to cite sources of information in your answer.\n",
    "7. After your answer, list \"References:\" in the format: [number] Author(s). Title. Journal, Year, Volume(Issue):Pages. DOI/URL\n",
    "\n",
    "Answer the question directly without any additional preamble or conclusion. Your response may include a \"References:\" section.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"Path/Question.txt\", \"r\", encoding='utf-8') as input_file, \\\n",
    "     open(\"Path/Question_model_test.txt\", \"w\", encoding='utf-8') as output_file:\n",
    "\n",
    "    # Read the problem line by line and process it\n",
    "    for i, question in enumerate(input_file, 1):\n",
    "        question = question.strip()\n",
    "        if not question: \n",
    "            continue\n",
    "\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Answer questions using LLM\n",
    "                refined_answer = chat_with_claude_opus(question, prompt1)  # chat_with_openai(question, prompt1)\n",
    "                \n",
    "                output_file.write(f\"User:\\n{question}\\n\\n\")\n",
    "                output_file.write(f\"### Response:\\n{refined_answer}\\n\\n\")\n",
    "                output_file.write(\"-\" * 50 + \"\\n\\n\") \n",
    "                \n",
    "                print(f\"Resolved issue {i}\")\n",
    "                break  \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"Request failed, error message: {str(e)}\")\n",
    "                print(f\"number of retries: {retry_count}/{max_retries}\")\n",
    "                \n",
    "                if retry_count == max_retries:\n",
    "                    print(f\"Maximum number of retries reached, skipping question{i}\")\n",
    "                    output_file.write(f\"User:\\n{question}\\n\\n\")\n",
    "                    output_file.write(\"### Response:\\n Handling failure\\n\")\n",
    "                    output_file.write(\"-\" * 50 + \"\\n\\n\")\n",
    "                else:\n",
    "                    wait_time = 2 ** (retry_count - 1)\n",
    "                    print(f\"Waiting {wait_time} seconds before trying again...\")\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
