{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import numpy as np\n",
    "! pip show anthropic\n",
    "! pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf032ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "ANTHROPIC_API_KEY=\"API_KEY\"\n",
    "OPENAI_API_KEY=\"API_KEY\"\n",
    "\n",
    "# Set your own IP to allow access\n",
    "proxy_url = 'http://----'\n",
    "proxy_port = 'xxxx' \n",
    "\n",
    "os.environ['http_proxy'] = f'{proxy_url}:{proxy_port}'\n",
    "os.environ['https_proxy'] = f'{proxy_url}:{proxy_port}'\n",
    "\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#API Usage\n",
    "client_claude = anthropic.Anthropic(api_key= ANTHROPIC_API_KEY)\n",
    "client_emb = OpenAI(api_key = OPENAI_API_KEY)\n",
    "client = OpenAI(api_key = OPENAI_API_KEY)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    return client_emb.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# Define chatbot\n",
    "def chat_with_claude_sonnet(prompt,system_prompt):\n",
    "    messages = [{\"role\": \"user\",\"content\": prompt}]\n",
    "    response = client_claude.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.5,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "    message = response.content[0].text\n",
    "\n",
    "    return message\n",
    "\n",
    "def chat_with_claude_opus(prompt,system_prompt):\n",
    "    messages = [{\"role\": \"user\",\"content\": prompt}]\n",
    "    response = client_claude.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=4000,\n",
    "        temperature=0.5,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "    message = response.content[0].text\n",
    "\n",
    "    return message\n",
    "\n",
    "def chat_with_claude_haiku(prompt,system_prompt):\n",
    "    messages = [{\"role\": \"user\",\"content\": prompt}]\n",
    "    response = client_claude.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=4000,\n",
    "        temperature=0.5,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "    message = response.content[0].text\n",
    "\n",
    "    return message\n",
    "\n",
    "def chat_with_openai(prompt, system_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import sys\n",
    "\n",
    "def chat_with_Llama3unsluth(prompt, system_prompt):\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\") # Modify the address provided by Lm-studio\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"Llama3_8B/unsluth\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        stream=True  # Enable streaming output\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end='', flush=True)\n",
    "            full_response += content\n",
    "    \n",
    "    print() \n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89926362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split query paragraphs into ab_word\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "\n",
    "TARGET_WORDS = 50  # Target number of words\n",
    "OVERLAP_WORDS = 30  # Number of overlapping words\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def split_into_overlapping_chunks(text, chunk_size, overlap):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "ab_word = []\n",
    "with open('Path/Artical_inf.csv', 'r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader)  # Skip the header row\n",
    "    for row in csv_reader:\n",
    "        if len(row) >= 8:\n",
    "            journal = row[1].strip()\n",
    "            pmid = row[4].strip()\n",
    "            pmcid = row[5].strip()\n",
    "            link = row[6].strip()\n",
    "            title = row[3].strip()\n",
    "            abstract = row[7].strip()\n",
    "            if len(abstract) >= 10:\n",
    "                chunks = split_into_overlapping_chunks(abstract, TARGET_WORDS, OVERLAP_WORDS)\n",
    "                for chunk in chunks:\n",
    "                    ab_word.append({\n",
    "                        'text': chunk,\n",
    "                        'metadata': {\n",
    "                            'journal': journal,\n",
    "                            'pmid': pmid,\n",
    "                            'pmcid': pmcid,\n",
    "                            'link': link\n",
    "                        },\n",
    "                        'document': title\n",
    "                    })\n",
    "\n",
    "print(\"The number of ab_word is:\", len(ab_word))\n",
    "print(\"Number of words in the example paragraph:\", [count_words(item['text']) for item in ab_word[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the embedded ab_word into the Json file\n",
    "def get_embedding_with_retry(text, max_retries=3, delay=20):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return get_embedding(text)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1: # If it is not the last attempt\n",
    "                print(f\"Error getting vector: {e}. wait 20s...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Failed after {max_retries} attempts. Skipping this item.\")\n",
    "                return None  # or return a default vector\n",
    "\n",
    "failed_items = [] # Storing failed fields\n",
    "\n",
    "with open('Path/ab_vector.json', 'w') as f:\n",
    "    f.write('[')  # Beginning JSON array\n",
    "    first_item = True\n",
    "    for i, item in enumerate(ab_word):\n",
    "        vector = get_embedding_with_retry(item['text'])\n",
    "        if vector is not None:\n",
    "            item['vector'] = vector  # Add the vector to the dictionary\n",
    "            if not first_item:\n",
    "                f.write(',')  # Add a comma before all elements except the first one\n",
    "            else:\n",
    "                first_item = False\n",
    "            json.dump(item, f)  # Write the dictionary to a JSON file\n",
    "        else:\n",
    "            failed_items.append(item) # Add the failed fields to the list\n",
    "        print(f\"Processing Items {i}\", end=\"\\r\")\n",
    "    f.write(']')  # End the JSON array\n",
    "print(\"\\nVector generation and writing completed\")\n",
    "\n",
    "# Write the failed fields to a separate JSON file\n",
    "with open('Path/fail_vector.json', 'w') as f:\n",
    "    json.dump(failed_items, f)\n",
    "\n",
    "print(f\"The failed fields have been written to the file: Path/fail_vector.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the vector set\n",
    "with open('Path/ab_vector.json', 'r') as file:\n",
    "    sentence_vector = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a vector number\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_most_similar(query_vector, embeddings, top_n):\n",
    "    similarities = cosine_similarity([query_vector], embeddings)[0]\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    return top_indices, similarities[top_indices]\n",
    "\n",
    "def find_deceleration_point(coefficients, scores_length):\n",
    "    first_derivative_coefficients = np.polyder(coefficients)\n",
    "    second_derivative_coefficients = np.polyder(first_derivative_coefficients)\n",
    "    roots = np.roots(second_derivative_coefficients)\n",
    "    real_roots = roots.real[abs(roots.imag) < 1e-5]\n",
    "    valid_roots = real_roots[(real_roots >= 0) & (real_roots <= scores_length - 1)]\n",
    "    \n",
    "    if len(valid_roots) > 0:\n",
    "        return int(np.min(valid_roots))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_similar_content(paragraph_vector, query, top_n=10):\n",
    "    word_vectors = [word['vector'] for word in paragraph_vector]\n",
    "    query_vector = get_embedding(query)\n",
    "    most_similar_idx, most_similar_scores = find_most_similar(query_vector, word_vectors, top_n)\n",
    "    \n",
    "    similar_items = []\n",
    "    for i in most_similar_idx:\n",
    "        item = paragraph_vector[i]\n",
    "        text = item['text']\n",
    "        metadata = item['metadata']\n",
    "        document = item['document']\n",
    "        \n",
    "        item_str = f\"Text: {text}\\nMetadata: {metadata}\\nDocument: {document}\\n\"\n",
    "        similar_items.append(item_str)\n",
    "    \n",
    "    return \"\\n\".join(similar_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9daf583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user input\n",
    "qury = input(\"Please enter your query: \")\n",
    "\n",
    "print(\"The query you entered is:\", qury)\n",
    "\n",
    "# entence_vector and extract_similar_content functions have been defined\n",
    "temp_str = extract_similar_content(sentence_vector, qury)  # Find similar vectors based on qury\n",
    "\n",
    "#Merge repeated sentences\n",
    "def remove_duplicates(temp_str):\n",
    "    lines = temp_str.split('\\n')\n",
    "    unique_lines = list(set(lines))\n",
    "    result_str = '\\n'.join(unique_lines)\n",
    "    return result_str\n",
    "# Deduplication output is deduplicated_str\n",
    "deduplicated_str = remove_duplicates(temp_str)\n",
    "\n",
    "combined_str = qury + \" \" + deduplicated_str\n",
    "\n",
    "print(\"The merged set:\", combined_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32200ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Usage\n",
    "prompt1 = \"\"\"Please identify the input paragraph. The first line of the paragraph is a question, which usually asks about a phenotype or gene. The remaining lines are answers related to this question. Requirements:\n",
    "1. Please identify these answers and judge whether they are related to the question based on the content. Only output new answers based on these answers with a little logic, and do not use your own knowledge base to supplement the answers.\n",
    "2. The output answers need to be answered in points, including (1) related gene families (2) related proteins (3) other related phenotypes (4) logical analysis (5) homologous genes in important crops (6) reference titles and DOI information\n",
    "3. The first sentence of the answer should be \"Based on your question, I have summarized the following relevant research information...\"\n",
    "4. Delete redundant sentences and make a good summary.\"\"\"\n",
    "\n",
    "part_output = chat_with_claude_opus(combined_str, prompt1)\n",
    "print(part_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ee9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Usage\n",
    "prompt1 = \"\"\"Please identify the input paragraph. The first line of the paragraph is a question, which usually asks about a phenotype or gene. The remaining lines are answers related to this question. Requirements:\n",
    "1. Please identify these answers and judge whether they are related to the question based on the content. Only output new answers based on these answers with a little logic, and do not use your own knowledge base to supplement the answers.\n",
    "2. The output answers need to be answered in points, including (1) related gene families (2) related proteins (3) other related phenotypes (4) logical analysis (5) homologous genes in important crops (6) reference titles and DOI information\n",
    "3. The first sentence of the answer should be \"Based on your question, I have summarized the following relevant research information...\"\n",
    "4. Delete redundant sentences and make a good summary.\"\"\"\n",
    "\n",
    "part_output = chat_with_Llama3unsluth(combined_str, prompt1)\n",
    "print(part_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
